{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "![metagenomics pipeline image](mg_pipeline.jpg)\n",
    "The pipeline for analyzing DNA sequences can be broken down into computational steps that makes the process easy to follow. Two different sequence data sets, 16s and shotgun, can be implemented into the ADToolbox for sequence analysis. An example of each will be demonstrated below.\n",
    "\n",
    "- The first step downloads sequence data from either a SRA sequence database or sequences collected from experimental data; though, if you already have your own sequencing data you can skip this step. \n",
    "\n",
    "- The sequence data will then be processed. Depending on the sequencing data (16s or shotgun) you must follow different steps that will be explained below. \n",
    "\n",
    "- The final output of each pipeline will be a json file that represents the relative abundance of microbial groups involved in the ADM model. \n",
    "\n",
    "Here, we will study two publicly available metagenomics studies, one for 16s & one for shotgun.\n",
    "\n",
    "- 16s data: https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP374594&o=acc_s%3Aa\n",
    "- shotgun data: https://www.ebi.ac.uk/ena/browser/view/PRJEB34458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16s Metagenomics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Downloading the 16s samples from SRA\n",
    "\n",
    "Using the link above, you must download the metadata table from SRA and extract all of the sample accesions and then download the sequence files for each sample using the code below. \n",
    "**Note** In this example the name of the metadata file is \"SraRunTable.txt\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from adtoolbox import core,configs,utils\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can download the metadata file using the following code:\n",
    "```bash\n",
    "!wget [LINK]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sra_metadatatable = pd.read_table(\"SraRunTable.txt\",delimiter=\",\",header='infer')\n",
    "dmc = configs.Metagenomics(adtoolbox_docker=configs.ADTOOLBOX_CONTAINERS[\"docker_arm64\"]) # \n",
    "mo = core.Metagenomics(dmc) # metadata object\n",
    "sample_accesions = sra_metadatatable[\"Run\"]\n",
    "base_seq_dir=pathlib.Path(\"./16s_samples_sequences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sample_accesions:\n",
    "    os.system(mo.seqs_from_sra(accession=i,target_dir=\"./16s_samples_sequences/\"+i)[0])\n",
    "# iterating through the sample accesions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process the samples with Qiime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In processing the samples with QIIMEII, there are multiple options. If you have QIIMEII installed on your computer, and it exists in the environment, and willing to run QIIME on your computer, you should choose \"None\" as your container option. If you do not have QIIMEII installed on your computer, you can either use docker or singularity depending on the availability of the software on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in sample_accesions:\n",
    "    process_samples = mo.run_qiime2_from_sra(\n",
    "                            read_1 = str(((base_seq_dir/i/i)/(i+\"_1.fastq\")).absolute()),\n",
    "                            read_2 = str(((base_seq_dir/i/i)/(i+\"_2.fastq\")).absolute()),\n",
    "                            sample_name = i,\n",
    "                            manifest_dir =str((base_seq_dir/i).absolute()) , #Where the manifest file will be saved\n",
    "                            workings_dir = str((base_seq_dir/i).absolute()), #Where the intermediate and output files will be saved\n",
    "                            save_manifest =True , #If you want to run the script use True\n",
    "                            container = 'docker',\n",
    "                            p_trunc_len=(\"240\",\"240\"),)\n",
    "    os.system(process_samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ASVs, we can align them to GTDB using the following code:\n",
    "(Note that the docker/singularity container must be selected according to your system and it is a different container than the one used for QIIMEII. The following is for mac with arm processors. Also note that the path to the files must be absolute.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc.adtoolbox_docker=configs.ADTOOLBOX_CONTAINERS[\"docker_arm64\"]\n",
    "## ALSO you can change the default vsearch similarity threshold by \n",
    "dmc.vsearch_similarity=0.95\n",
    "for i in base_seq_dir.rglob(\"*/dna-sequences.fasta\"):\n",
    "    os.system(mo.align_to_gtdb(str(i.absolute()),str(i.parent.absolute()),container = 'docker')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found our representative genomes we can download them. First, we must extract the genome IDs from the alignment file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes=set() # using sets to avoid duplicates\n",
    "for i in sample_accesions:\n",
    "    t=mo.get_genomes_from_gtdb_alignment(str(base_seq_dir/i))\n",
    "    genomes.update(t.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.97 similarity, we get about 450 representative genomes. Let's download them using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in genomes:\n",
    "    os.system(mo.download_genome(i,(base_seq_dir/\"genomes\").absolute(),container='docker')[0]) # usually you can run this without a container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded the representative genomes, we can align them to the ADToolbox protein database. To do this first we will use a helper method to extract all the genomes that we downloaded and their address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes_info=mo.extract_genome_info(base_dir=(base_seq_dir/\"genomes\").absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,address in genomes_info.items():\n",
    "    os.system(mo.align_genome_to_protein_db(address=address,outdir=str((base_seq_dir/\"genome_alignments\").absolute()),name=name,container='docker')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's extract the ec numbers from the alignment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_dict={}\n",
    "for i in (base_seq_dir/\"genome_alignments\").rglob(\"Alignment_Results_mmseq*\"):\n",
    "    ec_dict.update({\"_\".join(i.name.removeprefix(\"Alignment_Results_mmseq_\").split(\"_\")[:2]):mo.extract_ec_from_alignment(str(i.absolute()))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert these ec's to COD contribution of each genome to ADM microbial groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_dict={}\n",
    "for i in ec_dict.items():\n",
    "    cod_dict.update({i[0]:mo.get_cod_from_ec_counts(i[1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step we need to find the relative proportion of each microbial group in each sample. To do this we need to first get the feature abundances for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_cod={}\n",
    "for i in sample_accesions:\n",
    "    asv_map=mo.get_genomes_from_gtdb_alignment(str(base_seq_dir/i))\n",
    "    feature_table =pd.read_table((base_seq_dir/i/\"feature-table.tsv\"),delimiter=\"\\t\",skiprows=1)\n",
    "    feature_table[\"has_genome\"]=feature_table[\"#OTU ID\"].apply(lambda x: x in asv_map.keys())\n",
    "    feature_table=feature_table[feature_table[\"has_genome\"]]\n",
    "    feature_table.loc[:,i]=(feature_table[i]/feature_table[i].sum())\n",
    "    feature_table[\"genome\"]=feature_table[\"#OTU ID\"].apply(lambda x: asv_map[x])\n",
    "    feature_table=feature_table[[\"genome\",i]]\n",
    "    temp_cod=pd.DataFrame(np.matmul(pd.DataFrame.from_dict(cod_dict)[feature_table[\"genome\"]].to_numpy(),feature_table[i].to_numpy()),index=pd.DataFrame.from_dict(cod_dict).index)\n",
    "    temp_cod=temp_cod/temp_cod.sum()\n",
    "    temp_cod=temp_cod.to_dict()[0]\n",
    "    samples_cod.update({i:temp_cod})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"normalized_16s_samples_cod.json\",\"w\") as f:\n",
    "    json.dump(samples_cod,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of pipeline for 16s data. The final output is a json file that represents the relative abundance of microbial groups involved in the ADM model. This now can be used for modeling work. Check out the parameter tuning notebook for more information on how to use this json file for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shotgun Metagenomics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous example, we will download the shotgun data using the script below:\n",
    "\n",
    "```bash\n",
    "!wget [LINK]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download the sequence files for each sample using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from adtoolbox import core,configs,utils\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sra_metadatatable = pd.read_table(\"SraRunTable_Shotgun.txt\",delimiter=\"\\t\")\n",
    "# dmc = configs.Metagenomics(adtoolbox_docker=configs.ADTOOLBOX_CONTAINERS[\"docker_arm64\"]) # \n",
    "dmc = configs.Metagenomics(adtoolbox_docker=configs.ADTOOLBOX_CONTAINERS[\"docker_arm64\"]) # \n",
    "mo = core.Metagenomics(dmc) # metadata object\n",
    "sample_accesions = sra_metadatatable[\"run_accession\"]\n",
    "base_seq_dir=pathlib.Path(\"./Shotgun_samples_sequences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in sample_accesions:\n",
    "    os.system(mo.seqs_from_sra(sample,(base_seq_dir/sample).absolute(),container='docker')[0]) # use docker if you don't have SRAToolkit installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we downloaded the sequence files, we can analyze them. There are two common ways to analyze shotgun data, one is to assemble the short reads which finally yields MAGs with their relative abundance. ADToolbox does not offer assembly functionalitie. However, if you have  your MAGs with their relative abundances, the downstream part will be identicall to when you have 16s data with representative genome:\n",
    "align genomes to protein db -> extract ec from alignment -> convert ec to cod -> get COD relative abundance for each sample.\n",
    "However, if you prefer to map the short reads to the protein database directly, you can use an ADToolbox functionality. However, keep in mind that this process is resource intensive usually not possible on a local machine. Fortunately, ADToolbox offers a way to interact with HPCs that work with slurm. Even if you have a very large number of samples you can distribute the tasks in a way that suits your HPC. An example of how to use this functionality is shown below:\n",
    "###NOTE: These settings are taylored for Alpine HPCs at Colorado State Univeristy. You need to change the settings according to your HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_conf=configs.Utils(\n",
    "    slurm_executer=\"amilan\",\n",
    "    slurm_wall_time=\"24:00:00\",\n",
    "    slurm_cpus=\"12\",\n",
    "    slurm_memory=\"100G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a configs object. We have 12 metgenomics samples. We want to run each sample as a task on the HPC (all 12 in parallel). ADtoolbox does not accept paired end metagenomics samples. You have to merge the pared reads into one file. Let's say your files have a layout like this:\n",
    "\n",
    " ./Shotgun_samples_sequences/\\<SRAACCESSION>/\\<SRAACCESSION>_merged.fastq\n",
    " \n",
    "  There are plenty of tools to do this. Also, you can also simply merge the files but this is not recommended. Once you have the samples, you can submit the jobs to the HPC using the following code. Let's see an example for the first task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_addresses=[os.path.join(\"Shotgun_samples_sequences\",i,i+\"_merged.fastq\") for i in sample_accesions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --job-name=ERR3525312\n",
      "#SBATCH --partition=amilan\n",
      "#SBATCH --time=24:00:00\n",
      "#SBATCH --ntasks=12\n",
      "#SBATCH --mem=100G\n",
      "#SBATCH --output=ERR3525312.out.log\n",
      "\n",
      "\n",
      "singularity exec --bind /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312:/Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312 docker://parsaghadermazi/adtoolbox:x86 mmseqs createdb /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/ERR3525312_merged.fastq /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/ERR3525312_merged\n",
      "singularity exec --bind /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312:/Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312,/Users/parsaghadermarzi/Desktop/ADToolbox/Database:/Users/parsaghadermarzi/Desktop/ADToolbox/Database docker://parsaghadermazi/adtoolbox:x86 mmseqs search /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/ERR3525312_merged /Users/parsaghadermarzi/Desktop/ADToolbox/Database/protein_db_mmseqs /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/ERR3525312 /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/tmp\n",
      "singularity exec --bind /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312:/Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312,/Users/parsaghadermarzi/Desktop/ADToolbox/Database:/Users/parsaghadermarzi/Desktop/ADToolbox/Database docker://parsaghadermazi/adtoolbox:x86 mmseqs convertalis /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/ERR3525312_merged /Users/parsaghadermarzi/Desktop/ADToolbox/Database/protein_db_mmseqs /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/ERR3525312 /Users/parsaghadermarzi/Desktop/test_qiime/Shotgun_samples_sequences/ERR3525312/ERR3525312.tsv --format-mode 4\n",
      "\n",
      "\n",
      "\n",
      "echo 'Finished running ERR3525312'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commands=utils.generate_batch_script(\n",
    "        mo.align_short_reads_to_protein_db,\n",
    "        12,\n",
    "        input_series=list([sample_addresses,sample_accesions]),\n",
    "        input_var=[\"query_seq\",\"alignment_file_name\"],\n",
    "        container='singularity',\n",
    "        )\n",
    "for i in sample_accesions:\n",
    "    print(utils.wrap_for_slurm(command=commands[0],jobname=i,run=False,save=False,config=util_conf))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the jobs on the HPC using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands=utils.generate_batch_script(\n",
    "        mo.align_short_reads_to_protein_db,\n",
    "        12,\n",
    "        input_series=list([sample_addresses,sample_accesions]),\n",
    "        input_var=[\"query_seq\",\"alignment_file_name\"],\n",
    "        container='singularity',\n",
    "        )\n",
    "for ind,i in enumerate(sample_accesions):\n",
    "    utils.wrap_for_slurm(command=commands[ind],jobname=i,run=True,save=False,config=util_conf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the 16s analysis, we can now extract the ec numbers from the alignment file:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADtoolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
